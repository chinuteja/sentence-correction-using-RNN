{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "crude-aspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import pickle\n",
    "import fasttext\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
    "from nlpaug.util.file.download import DownloadUtil\n",
    "\n",
    "import nlpaug.augmenter.word as naw\n",
    "from tqdm import tqdm\n",
    "# pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "comfortable-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"final_data.csv\")\n",
    "data.drop(['Unnamed: 0'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acute-spirituality",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corupted_text</th>\n",
       "      <th>normal_text_input</th>\n",
       "      <th>normal_text_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U wan me to \"chop\" seat 4 u nt?</td>\n",
       "      <td>&lt;start&gt; Do you want me to reserve seat for you...</td>\n",
       "      <td>Do you want me to reserve seat for you or not?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yup. U reaching. We order some durian pastry a...</td>\n",
       "      <td>&lt;start&gt; Yeap. You reaching? We ordered some Du...</td>\n",
       "      <td>Yeap. You reaching? We ordered some Durian pas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They become more ex oredi... Mine is like 25.....</td>\n",
       "      <td>&lt;start&gt; They become more expensive already. Mi...</td>\n",
       "      <td>They become more expensive already. Mine is li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm thai. what do u do?</td>\n",
       "      <td>&lt;start&gt; I'm Thai. What do you do?</td>\n",
       "      <td>I'm Thai. What do you do? &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi! How did your week go? Haven heard from you...</td>\n",
       "      <td>&lt;start&gt; Hi! How did your week go? Haven't hear...</td>\n",
       "      <td>Hi! How did your week go? Haven't heard from y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       corupted_text  \\\n",
       "0                    U wan me to \"chop\" seat 4 u nt?   \n",
       "1  Yup. U reaching. We order some durian pastry a...   \n",
       "2  They become more ex oredi... Mine is like 25.....   \n",
       "3                            I'm thai. what do u do?   \n",
       "4  Hi! How did your week go? Haven heard from you...   \n",
       "\n",
       "                                   normal_text_input  \\\n",
       "0  <start> Do you want me to reserve seat for you...   \n",
       "1  <start> Yeap. You reaching? We ordered some Du...   \n",
       "2  <start> They become more expensive already. Mi...   \n",
       "3                  <start> I'm Thai. What do you do?   \n",
       "4  <start> Hi! How did your week go? Haven't hear...   \n",
       "\n",
       "                                  normal_text_output  \n",
       "0  Do you want me to reserve seat for you or not?...  \n",
       "1  Yeap. You reaching? We ordered some Durian pas...  \n",
       "2  They become more expensive already. Mine is li...  \n",
       "3                    I'm Thai. What do you do? <end>  \n",
       "4  Hi! How did your week go? Haven't heard from y...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "boring-matrix",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1973, 3)\n"
     ]
    }
   ],
   "source": [
    "train,test = train_test_split(data, test_size=0.01)\n",
    "print(train.shape)\n",
    "train.iloc[0]['normal_text_input']=str(train.iloc[0]['normal_text_input'])+' <end>'\n",
    "train.iloc[0]['normal_text_output']=str(train.iloc[0]['normal_text_output'])+' <end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "failing-paint",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 95.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer_source = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n', oov_token='ukn',lower=False)\n",
    "tokenizer_source.fit_on_texts(train['corupted_text'].values)\n",
    "\n",
    "tokenizer_target = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n', oov_token='ukn',lower=False)\n",
    "tokenizer_target.fit_on_texts(train['normal_text_input'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "democratic-roman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fast_text_model = fasttext.load_model('cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "prepared-birmingham",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size_encoder=(len(tokenizer_source.word_index)+1)\n",
    "vocab_size_decoder = (len(tokenizer_target.word_index)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "tough-potter",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_encoder = np.zeros((vocab_size_encoder,300))\n",
    "for word, i in tokenizer_source.word_index.items():\n",
    "    embedding_vector = fast_text_model.get_word_vector(word)\n",
    "    if embedding_vector is not None:\n",
    "    # words not found in embedding index will be all-zeros.\n",
    "       embedding_matrix_encoder[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fitted-found",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_decoder = np.zeros((vocab_size_decoder,300))\n",
    "for word, i in tokenizer_target.word_index.items():\n",
    "    embedding_vector = fast_text_model.get_word_vector(word)\n",
    "    if embedding_vector is not None:\n",
    "    # words not found in embedding index will be all-zeros.\n",
    "       embedding_matrix_decoder[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "committed-fleet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4615    (4615, 300)\n",
      "3599    (3599, 300)\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size_encoder,\"  \",embedding_matrix_encoder.shape)\n",
    "print(vocab_size_decoder,\"  \",embedding_matrix_decoder.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "beautiful-tribute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def convert_word_number(tokenizer,dataframe):\n",
    "    '''\n",
    "    heere we convert the each word to a digiti\n",
    "    '''\n",
    "    return tokenizer.texts_to_sequences(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "exotic-heater",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 101 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corupted_text_seq_train = convert_word_number(tokenizer_source,train[\"corupted_text\"])\n",
    "normal_text_seq_input_train = convert_word_number(tokenizer_target,train[\"normal_text_input\"])\n",
    "normal_text_seq_output_train = convert_word_number(tokenizer_target,train[\"normal_text_output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "suited-watch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 997 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corupted_text_seq_test = convert_word_number(tokenizer_source,test[\"corupted_text\"])\n",
    "normal_text_seq_input_test = convert_word_number(tokenizer_target,test[\"normal_text_input\"])\n",
    "normal_text_seq_output_test = convert_word_number(tokenizer_target,test[\"normal_text_output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "intellectual-watch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "finding maximum length of encoder input for padding values\n",
    "'''\n",
    "max_len = 0\n",
    "for i in corupted_text_seq_train:\n",
    "    if max_len < len(i):\n",
    "        max_len = len(i)\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "differential-borough",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "as we have decoder input seq and decoder ouput seq we need to find the which seq have maximumn length so that we can pad values \n",
    "'''\n",
    "\n",
    "max_len_dec_input = 0\n",
    "for i in normal_text_seq_input_train:\n",
    "    if max_len_dec_input < len(i):\n",
    "        max_len_dec_input = len(i)\n",
    "\n",
    "max_len_dec_output = 0\n",
    "for i in normal_text_seq_input_test:\n",
    "    if max_len_dec_output < len(i):\n",
    "        max_len_dec_output = len(i)\n",
    "max_len_dec = 0\n",
    "if max_len_dec_input < max_len_dec_output:\n",
    "    max_len_dec = max_len_dec_output\n",
    "else:\n",
    "    max_len_dec = max_len_dec_input\n",
    "max_len_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "impressive-quick",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pad_sequence(seq,length):\n",
    "    '''\n",
    "    here we are doing post padding to every sequence\n",
    "    '''\n",
    "    temp = pad_sequences(seq,maxlen=length,padding=\"post\")\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "radio-carpet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 48.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "source_seq_input_train = get_pad_sequence(corupted_text_seq_train,max_len)\n",
    "target_seq_input_train = get_pad_sequence(normal_text_seq_input_train,max_len_dec)\n",
    "target_seq_ouput_train = get_pad_sequence(normal_text_seq_output_train,max_len_dec)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "expensive-carolina",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 997 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "source_seq_input_test = get_pad_sequence(corupted_text_seq_test,max_len)\n",
    "target_seq_input_test = get_pad_sequence(normal_text_seq_input_test,max_len_dec)\n",
    "target_seq_ouput_test = get_pad_sequence(normal_text_seq_output_test,max_len_dec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "informational-theme",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    takes the input seq and returns the output,hidden and final state\n",
    "    '''\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units,input_length):\n",
    "        '''\n",
    "        here we initlaize the necessary attributes\n",
    "        '''\n",
    "\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.input_length = input_length\n",
    "        self.enc_units= enc_units\n",
    "        self.lstm_output = 0\n",
    "        self.lstm_state_h=0\n",
    "        self.lstm_state_c=0\n",
    "#         intialize embedding\n",
    "        self.embedding = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim, input_length=self.input_length,\n",
    "                           mask_zero=True, trainable=True, weights=[embedding_matrix_encoder], name=\"embedding_layer_encoder\")\n",
    "        self.lstm = Bidirectional(LSTM(self.enc_units, return_state=True, return_sequences=True, name='Encoder_LSTM2',dropout=0.2))\n",
    "        \n",
    "    def call(self, input_sentances, training=True):\n",
    "        '''\n",
    "        This function takes a sequence input\n",
    "        Pass the input_sequence input to the Embedding layer, Pass the embedding layer ouput to encoder_lstm\n",
    "         returns -- All encoder_outputs, last time steps hidden and cell state\n",
    "        '''\n",
    "        input_embedd= self.embedding(input_sentances)\n",
    "        self.lstm_output, lstm_state_h_f, lstm_state_c_f, lstm_state_h_b, lstm_state_c_b = self.lstm(input_embedd)\n",
    "        self.lstm_state_h = Concatenate()([lstm_state_h_f, lstm_state_h_b])\n",
    "        self.lstm_state_c = Concatenate()([lstm_state_c_f, lstm_state_c_b])\n",
    "        return self.lstm_output, self.lstm_state_h,self.lstm_state_c\n",
    "\n",
    "    def initialize_states(self, batch_size):\n",
    "         '''\n",
    "      Given a batch size it will return intial hidden state and intial cell state.\n",
    "      If batch size is 32- Hidden state is zeros of size [32,lstm_units], cell state zeros is of size [32,lstm_units]\n",
    "       '''\n",
    "        return tf.zeros((batch_size, self.enc_units)), tf.zeros((batch_size, self.enc_units))\n",
    "\n",
    "    def get_states(self):\n",
    "        return self.lstm_state_h,self.lstm_state_c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "colored-verification",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    attention layer\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,scoring_function, att_units):\n",
    "        super(Attention,self).__init__()\n",
    "        self.scoring_function = scoring_function\n",
    "        self.att_units = att_units\n",
    "        if self.scoring_function=='dot':\n",
    "            self.dot = tf.keras.layers.Dot(axes=[2,2])\n",
    "        elif scoring_function == 'general':\n",
    "            self.WG = Dense(self.att_units)\n",
    "        elif scoring_function == 'concat':\n",
    "            self.W1 = Dense(att_units)\n",
    "            self.W2 = Dense(att_units)\n",
    "            self.V = Dense(1)\n",
    "\n",
    "    def call(self,inp):        \n",
    "        decoder_hidden_state,encoder_output=inp\n",
    "        decoder_hidden_state = tf.expand_dims(decoder_hidden_state, axis=1)    \n",
    "        if self.scoring_function == 'dot':\n",
    "            score = self.dot([encoder_output, decoder_hidden_state])\n",
    "        elif self.scoring_function == 'general':          \n",
    "            score = tf.keras.layers.Dot(axes=[2, 2])([self.WG(encoder_output), decoder_hidden_state])\n",
    "        elif self.scoring_function == 'concat':\n",
    "            score = self.V(tf.nn.tanh(self.W1(decoder_hidden_state) + self.W2(encoder_output)))\n",
    "        attention_weights = Softmax(axis=1)(score)     \n",
    "        context_vector = attention_weights * encoder_output\n",
    "        # shape = (batch_size, dec lstm units)\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "governing-thirty",
   "metadata": {},
   "outputs": [],
   "source": [
    "class One_Step_Decoder(tf.keras.Model):\n",
    "    def __init__(self, tar_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units): \n",
    "        super(One_Step_Decoder,self).__init__()\n",
    "        self.vocab_size = tar_vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.input_length = input_length\n",
    "        self.dec_units = dec_units\n",
    "        self.score_fun = score_fun\n",
    "        self.att_units = att_units\n",
    "        self.embedding = Embedding(self.vocab_size, self.embedding_dim, trainable=True, weights=[embedding_matrix_decoder], input_length=self.input_length, mask_zero=True, name=\"Att_Dec_Embedding\")\n",
    "        self.lstm = LSTM(self.dec_units, return_sequences=True, return_state=True, name=\"Att_Dec_LSTM\",dropout=0.2)\n",
    "        self.fc = Dense(self.vocab_size)\n",
    "        self.attention = Attention(self.score_fun,self.att_units)\n",
    "\n",
    "    def call(self, inputs2):\n",
    "         # One step decoder mechanisim step by step:\n",
    "        # A. Pass the input_to_decoder to the embedding layer and then get the output(batch_size,1,embedding_dim)\n",
    "        #B. Using the encoder_output and decoder hidden state, compute the context vector.\n",
    "        # context_vector = tf.expand_dims(context_vector,axis=1)\n",
    "        # C. Concat the context vector with the step A output\n",
    "                # D. Pass the Step-C output to LSTM/GRU and get the decoder output and states(hidden and cell state)\n",
    "#         print(\"state_h\",state_h.shape)\n",
    "#         print(\"state_c\",state_c.shape)\n",
    " # E. Pass the decoder output to dense layer(vocab size) and store the result into output.\n",
    "    # F. Return the states from step D, output from Step E, attention weights from Step -B\n",
    "        input_to_decoder, encoder_output, state_h, state_c=inputs2\n",
    "        embedded_input = self.embedding(input_to_decoder)\n",
    "        context_vector, attention_weights = self.attention((state_h, encoder_output))\n",
    "        decoder_input = tf.concat([tf.expand_dims(context_vector, 1), embedded_input], axis=-1)\n",
    "        decoder_output, dec_state_h, dec_state_c = self.lstm(decoder_input, initial_state=[state_h, state_c])\n",
    "        decoder_output = tf.reshape(decoder_output, (-1, decoder_output.shape[2]))\n",
    "        output = self.fc(decoder_output)\n",
    "        return output, dec_state_h, dec_state_c, attention_weights, context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "flying-increase",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):  \n",
    "    def __init__(self,out_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.vocab_size = out_vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.input_length = input_length\n",
    "        self.dec_units = dec_units\n",
    "        self.score_fun = score_fun\n",
    "        self.att_units = att_units\n",
    "        self.onestepdecoder = One_Step_Decoder(self.vocab_size, self.embedding_dim, self.input_length,\n",
    "                                            self.dec_units, self.score_fun, self.att_units)\n",
    "    @tf.function\n",
    "    def call(self, inputs): \n",
    "        input_to_decoder,encoder_output,decoder_hidden_state,decoder_cell_state=inputs\n",
    "        all_outputs = tf.TensorArray(tf.float32, size=input_to_decoder.shape[1])\n",
    "        for timestep in range(input_to_decoder.shape[1]):\n",
    "            output, decoder_hidden_state, decoder_cell_state, attention_weights, context_vector = self.onestepdecoder((\n",
    "                input_to_decoder[:, timestep:timestep+1], encoder_output, decoder_hidden_state, decoder_cell_state))\n",
    "            all_outputs = all_outputs.write(timestep, output)\n",
    "        all_outputs = tf.transpose(all_outputs.stack(), [1,0,2])\n",
    "        return all_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fuzzy-workshop",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_decoder(Model):\n",
    "    def __init__(self, encoder_inputs_length,decoder_inputs_length,batch_size,score_fun):\n",
    "        super().__init__() # https://stackoverflow.com/a/27134600/4084039\n",
    "        self.batch_size=batch_size\n",
    "        self.encoder = Encoder(vocab_size= vocab_size_encoder, embedding_dim=300, input_length=encoder_inputs_length, enc_units=128)\n",
    "        self.decoder = Decoder(out_vocab_size= vocab_size_decoder, embedding_dim=300, input_length=decoder_inputs_length, dec_units=256, score_fun=score_fun, att_units=256)\n",
    "    @tf.function\n",
    "    def call(self, data):\n",
    "        input,output = data[0], data[1]\n",
    "        enc_initial_states = self.encoder.initialize_states(self.batch_size)\n",
    "        encoder_output, encoder_h, encoder_c = self.encoder(input)\n",
    "        decoder_output   = self.decoder((output, encoder_output, encoder_h, encoder_c))\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "micro-reach",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Encoder_decoder(encoder_inputs_length=max_len,decoder_inputs_length=max_len_dec,batch_size=16,score_fun = \"concat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "spare-execution",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.98, patience=3, mode=\"min\", verbose=1)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0,\n",
    "    patience=5,\n",
    "    verbose=0,\n",
    "    mode=\"auto\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "aquatic-lambda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_lossfunction(real, pred):\n",
    "      # Custom loss function that will not consider the loss for padded zeros.\n",
    "    #https://www.tensorflow.org/tutorials/text/nmt_with_attention#define_the_optimizer_and_the_loss_function\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')    \n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(optimizer=optimizer,loss=custom_lossfunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "corrected-cycling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "111/111 [==============================] - 71s 643ms/step - loss: 2.2244 - val_loss: 2.1952 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "111/111 [==============================] - 60s 542ms/step - loss: 2.0185 - val_loss: 2.1426 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "111/111 [==============================] - 60s 544ms/step - loss: 1.9206 - val_loss: 2.0832 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "111/111 [==============================] - 66s 591ms/step - loss: 1.8132 - val_loss: 2.0181 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "111/111 [==============================] - 74s 664ms/step - loss: 1.7100 - val_loss: 1.9669 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "111/111 [==============================] - 69s 619ms/step - loss: 1.6134 - val_loss: 1.9336 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "111/111 [==============================] - 64s 572ms/step - loss: 1.5197 - val_loss: 1.8930 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "111/111 [==============================] - 65s 587ms/step - loss: 1.4269 - val_loss: 1.8676 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "111/111 [==============================] - 60s 543ms/step - loss: 1.3393 - val_loss: 1.8302 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "111/111 [==============================] - 61s 551ms/step - loss: 1.2498 - val_loss: 1.8208 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "111/111 [==============================] - 62s 559ms/step - loss: 1.1645 - val_loss: 1.7910 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "111/111 [==============================] - 61s 549ms/step - loss: 1.0808 - val_loss: 1.7750 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "111/111 [==============================] - 60s 543ms/step - loss: 0.9964 - val_loss: 1.7838 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "111/111 [==============================] - 61s 551ms/step - loss: 0.9211 - val_loss: 1.7351 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "111/111 [==============================] - 61s 553ms/step - loss: 0.8390 - val_loss: 1.7331 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "111/111 [==============================] - 61s 547ms/step - loss: 0.7726 - val_loss: 1.7114 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "111/111 [==============================] - 61s 547ms/step - loss: 0.7090 - val_loss: 1.7162 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "111/111 [==============================] - 61s 548ms/step - loss: 0.6439 - val_loss: 1.6999 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "111/111 [==============================] - 61s 549ms/step - loss: 0.5972 - val_loss: 1.7067 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "111/111 [==============================] - 61s 548ms/step - loss: 0.5267 - val_loss: 1.6978 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "111/111 [==============================] - 60s 543ms/step - loss: 0.4620 - val_loss: 1.6898 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "111/111 [==============================] - 60s 538ms/step - loss: 0.4180 - val_loss: 1.6848 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "111/111 [==============================] - 60s 537ms/step - loss: 0.3797 - val_loss: 1.6868 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "111/111 [==============================] - 60s 537ms/step - loss: 0.3405 - val_loss: 1.7014 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.3058\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0009800000465475024.\n",
      "111/111 [==============================] - 61s 546ms/step - loss: 0.3058 - val_loss: 1.7034 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "111/111 [==============================] - 60s 545ms/step - loss: 0.2700 - val_loss: 1.7089 - lr: 9.8000e-04\n",
      "Epoch 27/100\n",
      "111/111 [==============================] - 61s 546ms/step - loss: 0.2526 - val_loss: 1.7423 - lr: 9.8000e-04\n",
      "Wall time: 30min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history=model.fit([source_seq_input_train, target_seq_input_train],target_seq_ouput_train, epochs=100,batch_size=16,validation_split = 0.1,callbacks=[reduce_lr,early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "cooked-safety",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_sentence):\n",
    "\n",
    "  input_sequence=tokenizer_source.texts_to_sequences([input_sentence])\n",
    "  inputs=pad_sequences(input_sequence, maxlen=max_len, padding='post')\n",
    "  inputs=tf.convert_to_tensor(inputs)\n",
    "  result=''\n",
    "  units=128\n",
    "  hidden=[tf.zeros((1,units))]\n",
    "  encoder_output,hidden_state,cell_state=model.encoder(inputs)\n",
    "  dec_hidden=hidden_state\n",
    "  dec_input=tf.expand_dims([tokenizer_target.word_index['<start>']],0)\n",
    "  for t in range(40):\n",
    "      predictions,dec_hidden,cell_state,attention_weights,context_vector=model.decoder.onestepdecoder((dec_input,encoder_output,dec_hidden,cell_state))\n",
    "\n",
    "      predicted_id=tf.argmax(predictions[0]).numpy()\n",
    "      result+=tokenizer_target.index_word[predicted_id]+' '\n",
    "      if tokenizer_target.word_index['<end>']==predicted_id:\n",
    "          return result\n",
    "      dec_input= tf.expand_dims([predicted_id],0)\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "alien-value",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Rain,can u call mi nw?97482959.\n",
      "Prediction: Rain you call you now <end> \n",
      "Actual: Rain, can you call me now? 97482959. <end>\n",
      "****************************************************************************************************\n",
      "Input: Roy intro pls\n",
      "Prediction: Mimi40 please introduce please <end> \n",
      "Actual: Roy introduce please. <end>\n",
      "****************************************************************************************************\n",
      "Input: Dear.... Miss you.\n",
      "Prediction: Hey how you <end> \n",
      "Actual: Dear. Miss you. <end>\n",
      "****************************************************************************************************\n",
      "Input: What're you doing tonight?\n",
      "Prediction: Jess you are here <end> \n",
      "Actual: What're you doing tonight? <end>\n",
      "****************************************************************************************************\n",
      "Input: hi roy ! Intro pls ..... Pls sms at 016 5419814\n",
      "Prediction: Hi Roy care please introduce Please chat anyone <end> \n",
      "Actual: Hi Roy! Introduce please. Please SMS at 016 5419814. <end>\n",
      "****************************************************************************************************\n",
      "Input: U having ur lunch at home or in school? I'm having lunch now.\n",
      "Prediction: You are you free at home or in school or not at home now <end> \n",
      "Actual: You are having your lunch at home or in school? I'm having lunch now. <end>\n",
      "****************************************************************************************************\n",
      "Input: hai sine? intro pls?\n",
      "Prediction: Hi JOY introduce please <end> \n",
      "Actual: Hi Sine? Introduce please? <end>\n",
      "****************************************************************************************************\n",
      "Input: Okie...Sian rite? Hafta go back to e hw days...Wat u taking? N how's yr timetable? Anyway u tink i shld bring it to c doc? My sis say will hef disease 1...\n",
      "Prediction: Ok since right Have you back to the way Because what you are born And when your other way else do you want to see twice My sister say will scold me asap <end> \n",
      "Actual: Ok. Sian right? I have to go back to the whole days. What are you taking? And how's your timetable? Anyway you think I should bring it to see doctor? My sister says it will have disease. <end>\n",
      "****************************************************************************************************\n",
      "Input: Hi jeff! Can intro pls? Phone no if any? My no is 0166305681 ok?\n",
      "Prediction: Hi Everybody can I come please introduce number if if no point an number ok <end> \n",
      "Actual: Hi jeff! Can introduce please? Phone number if any? My no is 0166305681 ok? <end>\n",
      "****************************************************************************************************\n",
      "Input: You have to bring your's newspaper 12 behind.\n",
      "Prediction: You have to bring 3 months for quarantine <end> \n",
      "Actual: You have to bring your newspaper to behind. <end>\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,10):\n",
    "    input_sentence=test[\"corupted_text\"].iloc[i]\n",
    "    print('Input:',input_sentence)\n",
    "    print('Prediction:',predict(input_sentence))\n",
    "    print('Actual:',test[\"normal_text_output\"].iloc[i])\n",
    "    print('*'*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
