{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "elementary-reporter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import pickle\n",
    "import fasttext\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
    "from nlpaug.util.file.download import DownloadUtil\n",
    "\n",
    "import nlpaug.augmenter.word as naw\n",
    "from tqdm import tqdm\n",
    "# pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "collectible-miami",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"normalized_data.csv\")\n",
    "data.drop(['Unnamed: 0',\"Source_len\",\"Target_len\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "happy-recipe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corupted_text</th>\n",
       "      <th>normal_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U wan me to \"chop\" seat 4 u nt?</td>\n",
       "      <td>Do you want me to reserve seat for you or not?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yup. U reaching. We order some durian pastry a...</td>\n",
       "      <td>Yeap. You reaching? We ordered some Durian pas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They become more ex oredi... Mine is like 25.....</td>\n",
       "      <td>They become more expensive already. Mine is li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm thai. what do u do?</td>\n",
       "      <td>I'm Thai. What do you do?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi! How did your week go? Haven heard from you...</td>\n",
       "      <td>Hi! How did your week go? Haven't heard from y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       corupted_text  \\\n",
       "0                    U wan me to \"chop\" seat 4 u nt?   \n",
       "1  Yup. U reaching. We order some durian pastry a...   \n",
       "2  They become more ex oredi... Mine is like 25.....   \n",
       "3                            I'm thai. what do u do?   \n",
       "4  Hi! How did your week go? Haven heard from you...   \n",
       "\n",
       "                                         normal_text  \n",
       "0     Do you want me to reserve seat for you or not?  \n",
       "1  Yeap. You reaching? We ordered some Durian pas...  \n",
       "2  They become more expensive already. Mine is li...  \n",
       "3                          I'm Thai. What do you do?  \n",
       "4  Hi! How did your week go? Haven't heard from y...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "american-expression",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1993/1993 [00:12<00:00, 160.57it/s]\n"
     ]
    }
   ],
   "source": [
    "normal_text = list(data[\"normal_text\"].values)\n",
    "aug_text = []\n",
    "aug = naw.SynonymAug()\n",
    "for sentence in tqdm(normal_text):\n",
    "    aug_text.append(aug.augment(sentence))\n",
    "df_synaug = pd.DataFrame({\"corupted_text\":aug_text,\"normal_text\":aug_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "continent-shoot",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1993/1993 [00:00<00:00, 3138.82it/s]\n"
     ]
    }
   ],
   "source": [
    "normal_text = list(data[\"normal_text\"].values)\n",
    "aug_text = []\n",
    "aug = naw.RandomWordAug()\n",
    "for sentence in tqdm(normal_text):\n",
    "    aug_text.append(aug.augment(sentence))\n",
    "df_random = pd.DataFrame({\"corupted_text\":aug_text,\"normal_text\":aug_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "olympic-static",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([data,df_synaug,df_random])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "international-plastic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5979, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "guilty-potato",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"normal_text_input\"] = \"<start>\"+df[\"normal_text\"].astype(str)\n",
    "df[\"normal_text_output\"] = df[\"normal_text\"].astype(str) + \"<end>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "indirect-liquid",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"normal_text\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "varying-specification",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5979, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "configured-carry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5919, 3)\n"
     ]
    }
   ],
   "source": [
    "train,test = train_test_split(df, test_size=0.01)\n",
    "print(train.shape)\n",
    "train.iloc[0]['normal_text_input']=str(train.iloc[0]['normal_text_input'])+' <end>'\n",
    "train.iloc[0]['normal_text_output']=str(train.iloc[0]['normal_text_output'])+' <end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "hawaiian-resident",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 598 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer_source = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n', oov_token='ukn',lower=False)\n",
    "tokenizer_source.fit_on_texts(train['corupted_text'].values)\n",
    "\n",
    "tokenizer_target = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n', oov_token='ukn',lower=False)\n",
    "tokenizer_target.fit_on_texts(train['normal_text_input'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "beautiful-flour",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19.5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fast_text_model = fasttext.load_model('cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "conventional-tribune",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size_encoder=(len(tokenizer_source.word_index)+1)\n",
    "vocab_size_decoder = (len(tokenizer_target.word_index)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "attached-remedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_encoder = np.zeros((vocab_size_encoder,300))\n",
    "for word, i in tokenizer_source.word_index.items():\n",
    "    embedding_vector = fast_text_model.get_word_vector(word)\n",
    "    if embedding_vector is not None:\n",
    "    # words not found in embedding index will be all-zeros.\n",
    "       embedding_matrix_encoder[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "short-canon",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_decoder = np.zeros((vocab_size_decoder,300))\n",
    "for word, i in tokenizer_target.word_index.items():\n",
    "    embedding_vector = fast_text_model.get_word_vector(word)\n",
    "    if embedding_vector is not None:\n",
    "    # words not found in embedding index will be all-zeros.\n",
    "       embedding_matrix_decoder[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "placed-senate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7259    (7259, 300)\n",
      "5866    (5866, 300)\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size_encoder,\"  \",embedding_matrix_encoder.shape)\n",
    "print(vocab_size_decoder,\"  \",embedding_matrix_decoder.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "composite-utilization",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def convert_word_number(tokenizer,dataframe):\n",
    "    '''\n",
    "    heere we convert the each word to a digiti\n",
    "    '''\n",
    "    return tokenizer.texts_to_sequences(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "noble-county",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 637 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corupted_text_seq_train = convert_word_number(tokenizer_source,train[\"corupted_text\"])\n",
    "normal_text_seq_input_train = convert_word_number(tokenizer_target,train[\"normal_text_input\"])\n",
    "normal_text_seq_output_train = convert_word_number(tokenizer_target,train[\"normal_text_output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ultimate-suffering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.78 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corupted_text_seq_test = convert_word_number(tokenizer_source,test[\"corupted_text\"])\n",
    "normal_text_seq_input_test = convert_word_number(tokenizer_target,test[\"normal_text_input\"])\n",
    "normal_text_seq_output_test = convert_word_number(tokenizer_target,test[\"normal_text_output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "hungry-graphic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "finding maximum length of encoder input for padding values\n",
    "'''\n",
    "max_len = 0\n",
    "for i in corupted_text_seq_train:\n",
    "    if max_len < len(i):\n",
    "        max_len = len(i)\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "unnecessary-bunny",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "as we have decoder input seq and decoder ouput seq we need to find the which seq have maximumn length so that we can pad values \n",
    "'''\n",
    "\n",
    "max_len_dec_input = 0\n",
    "for i in normal_text_seq_input_train:\n",
    "    if max_len_dec_input < len(i):\n",
    "        max_len_dec_input = len(i)\n",
    "\n",
    "max_len_dec_output = 0\n",
    "for i in normal_text_seq_input_test:\n",
    "    if max_len_dec_output < len(i):\n",
    "        max_len_dec_output = len(i)\n",
    "max_len_dec = 0\n",
    "if max_len_dec_input < max_len_dec_output:\n",
    "    max_len_dec = max_len_dec_output\n",
    "else:\n",
    "    max_len_dec = max_len_dec_input\n",
    "max_len_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "clinical-spider",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pad_sequence(seq,length):\n",
    "    '''\n",
    "    here we are doing post padding to every sequence\n",
    "    '''\n",
    "    temp = pad_sequences(seq,maxlen=length,padding=\"post\")\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "alien-waste",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 242 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "source_seq_input_train = get_pad_sequence(corupted_text_seq_train,max_len)\n",
    "target_seq_input_train = get_pad_sequence(normal_text_seq_input_train,max_len_dec)\n",
    "target_seq_ouput_train = get_pad_sequence(normal_text_seq_output_train,max_len_dec)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "trained-tennis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "source_seq_input_test = get_pad_sequence(corupted_text_seq_test,max_len)\n",
    "target_seq_input_test = get_pad_sequence(normal_text_seq_input_test,max_len_dec)\n",
    "target_seq_ouput_test = get_pad_sequence(normal_text_seq_output_test,max_len_dec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "apart-verification",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    takes the input seq and returns the output,hidden and final state\n",
    "    '''\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units,input_length):\n",
    "        '''\n",
    "        here we initlaize the necessary attributes\n",
    "        '''\n",
    "\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.input_length = input_length\n",
    "        self.enc_units= enc_units\n",
    "        self.lstm_output = 0\n",
    "        self.lstm_state_h=0\n",
    "        self.lstm_state_c=0\n",
    "#         intialize embedding\n",
    "        self.embedding = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim, input_length=self.input_length,\n",
    "                           mask_zero=True, trainable=True, weights=[embedding_matrix_encoder], name=\"embedding_layer_encoder\")\n",
    "        self.lstm = Bidirectional(LSTM(self.enc_units, return_state=True, return_sequences=True, name='Encoder_LSTM2',dropout=0.2))\n",
    "        \n",
    "    def call(self, input_sentances, training=True):\n",
    "        '''\n",
    "        This function takes a sequence input\n",
    "        Pass the input_sequence input to the Embedding layer, Pass the embedding layer ouput to encoder_lstm\n",
    "         returns -- All encoder_outputs, last time steps hidden and cell state\n",
    "        '''\n",
    "        input_embedd= self.embedding(input_sentances)\n",
    "        self.lstm_output, lstm_state_h_f, lstm_state_c_f, lstm_state_h_b, lstm_state_c_b = self.lstm(input_embedd)\n",
    "        self.lstm_state_h = Concatenate()([lstm_state_h_f, lstm_state_h_b])\n",
    "        self.lstm_state_c = Concatenate()([lstm_state_c_f, lstm_state_c_b])\n",
    "        return self.lstm_output, self.lstm_state_h,self.lstm_state_c\n",
    "\n",
    "    def initialize_states(self, batch_size):\n",
    "        \n",
    "        '''\n",
    "       Given a batch size it will return intial hidden state and intial cell state.\n",
    "      If batch size is 32- Hidden state is zeros of size [32,lstm_units], cell state zeros is of size [32,lstm_units]\n",
    "         '''\n",
    "        \n",
    "     \n",
    "        return tf.zeros((batch_size, self.enc_units)), tf.zeros((batch_size, self.enc_units))\n",
    "\n",
    "    def get_states(self):\n",
    "        return self.lstm_state_h,self.lstm_state_c\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "authentic-islam",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    attention layer\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,scoring_function, att_units):\n",
    "        super(Attention,self).__init__()\n",
    "        self.scoring_function = scoring_function\n",
    "        self.att_units = att_units\n",
    "        if self.scoring_function=='dot':\n",
    "            self.dot = tf.keras.layers.Dot(axes=[2,2])\n",
    "        elif scoring_function == 'general':\n",
    "            self.WG = Dense(self.att_units)\n",
    "        elif scoring_function == 'concat':\n",
    "            self.W1 = Dense(att_units)\n",
    "            self.W2 = Dense(att_units)\n",
    "            self.V = Dense(1)\n",
    "\n",
    "    def call(self,inp):        \n",
    "        decoder_hidden_state,encoder_output=inp\n",
    "        decoder_hidden_state = tf.expand_dims(decoder_hidden_state, axis=1)    \n",
    "        if self.scoring_function == 'dot':\n",
    "            score = self.dot([encoder_output, decoder_hidden_state])\n",
    "        elif self.scoring_function == 'general':          \n",
    "            score = tf.keras.layers.Dot(axes=[2, 2])([self.WG(encoder_output), decoder_hidden_state])\n",
    "        elif self.scoring_function == 'concat':\n",
    "            score = self.V(tf.nn.tanh(self.W1(decoder_hidden_state) + self.W2(encoder_output)))\n",
    "        attention_weights = Softmax(axis=1)(score)     \n",
    "        context_vector = attention_weights * encoder_output\n",
    "        # shape = (batch_size, dec lstm units)\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "stunning-buying",
   "metadata": {},
   "outputs": [],
   "source": [
    "class One_Step_Decoder(tf.keras.Model):\n",
    "    def __init__(self, tar_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units): \n",
    "        super(One_Step_Decoder,self).__init__()\n",
    "        self.vocab_size = tar_vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.input_length = input_length\n",
    "        self.dec_units = dec_units\n",
    "        self.score_fun = score_fun\n",
    "        self.att_units = att_units\n",
    "        self.embedding = Embedding(self.vocab_size, self.embedding_dim, trainable=True, weights=[embedding_matrix_decoder], input_length=self.input_length, mask_zero=True, name=\"Att_Dec_Embedding\")\n",
    "        self.lstm = LSTM(self.dec_units, return_sequences=True, return_state=True, name=\"Att_Dec_LSTM\",dropout=0.2)\n",
    "        self.fc = Dense(self.vocab_size)\n",
    "        self.attention = Attention(self.score_fun,self.att_units)\n",
    "\n",
    "    def call(self, inputs2):\n",
    "         # One step decoder mechanisim step by step:\n",
    "        # A. Pass the input_to_decoder to the embedding layer and then get the output(batch_size,1,embedding_dim)\n",
    "        #B. Using the encoder_output and decoder hidden state, compute the context vector.\n",
    "        # context_vector = tf.expand_dims(context_vector,axis=1)\n",
    "        # C. Concat the context vector with the step A output\n",
    "                # D. Pass the Step-C output to LSTM/GRU and get the decoder output and states(hidden and cell state)\n",
    "#         print(\"state_h\",state_h.shape)\n",
    "#         print(\"state_c\",state_c.shape)\n",
    " # E. Pass the decoder output to dense layer(vocab size) and store the result into output.\n",
    "    # F. Return the states from step D, output from Step E, attention weights from Step -B\n",
    "        input_to_decoder, encoder_output, state_h, state_c=inputs2\n",
    "        embedded_input = self.embedding(input_to_decoder)\n",
    "        context_vector, attention_weights = self.attention((state_h, encoder_output))\n",
    "        decoder_input = tf.concat([tf.expand_dims(context_vector, 1), embedded_input], axis=-1)\n",
    "        decoder_output, dec_state_h, dec_state_c = self.lstm(decoder_input, initial_state=[state_h, state_c])\n",
    "        decoder_output = tf.reshape(decoder_output, (-1, decoder_output.shape[2]))\n",
    "        output = self.fc(decoder_output)\n",
    "        return output, dec_state_h, dec_state_c, attention_weights, context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "editorial-richards",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):  \n",
    "    def __init__(self,out_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.vocab_size = out_vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.input_length = input_length\n",
    "        self.dec_units = dec_units\n",
    "        self.score_fun = score_fun\n",
    "        self.att_units = att_units\n",
    "        self.onestepdecoder = One_Step_Decoder(self.vocab_size, self.embedding_dim, self.input_length,\n",
    "                                            self.dec_units, self.score_fun, self.att_units)\n",
    "    @tf.function\n",
    "    def call(self, inputs): \n",
    "        input_to_decoder,encoder_output,decoder_hidden_state,decoder_cell_state=inputs\n",
    "        all_outputs = tf.TensorArray(tf.float32, size=input_to_decoder.shape[1])\n",
    "        for timestep in range(input_to_decoder.shape[1]):\n",
    "            output, decoder_hidden_state, decoder_cell_state, attention_weights, context_vector = self.onestepdecoder((\n",
    "                input_to_decoder[:, timestep:timestep+1], encoder_output, decoder_hidden_state, decoder_cell_state))\n",
    "            all_outputs = all_outputs.write(timestep, output)\n",
    "        all_outputs = tf.transpose(all_outputs.stack(), [1,0,2])\n",
    "        return all_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "elementary-enzyme",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_decoder(Model):\n",
    "    def __init__(self, encoder_inputs_length,decoder_inputs_length,batch_size,score_fun):\n",
    "        super().__init__() # https://stackoverflow.com/a/27134600/4084039\n",
    "        self.batch_size=batch_size\n",
    "        self.encoder = Encoder(vocab_size= vocab_size_encoder, embedding_dim=300, input_length=encoder_inputs_length, enc_units=128)\n",
    "        self.decoder = Decoder(out_vocab_size= vocab_size_decoder, embedding_dim=300, input_length=decoder_inputs_length, dec_units=256, score_fun=score_fun, att_units=256)\n",
    "    @tf.function\n",
    "    def call(self, data):\n",
    "        input,output = data[0], data[1]\n",
    "        enc_initial_states = self.encoder.initialize_states(self.batch_size)\n",
    "        encoder_output, encoder_h, encoder_c = self.encoder(input)\n",
    "        decoder_output   = self.decoder((output, encoder_output, encoder_h, encoder_c))\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "blessed-stadium",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Encoder_decoder(encoder_inputs_length=max_len,decoder_inputs_length=max_len_dec,batch_size=16,score_fun = \"concat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "activated-executive",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.98, patience=3, mode=\"min\", verbose=1)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0,\n",
    "    patience=5,\n",
    "    verbose=0,\n",
    "    mode=\"auto\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "whole-mills",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_lossfunction(real, pred):\n",
    "      # Custom loss function that will not consider the loss for padded zeros.\n",
    "    #https://www.tensorflow.org/tutorials/text/nmt_with_attention#define_the_optimizer_and_the_loss_function\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')    \n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(optimizer=optimizer,loss=custom_lossfunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "interpreted-phoenix",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "333/333 [==============================] - 371s 1s/step - loss: 1.4737 - val_loss: 1.1096 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "333/333 [==============================] - 279s 839ms/step - loss: 0.9117 - val_loss: 0.7554 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "333/333 [==============================] - 283s 850ms/step - loss: 0.6171 - val_loss: 0.5623 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "333/333 [==============================] - 281s 843ms/step - loss: 0.4425 - val_loss: 0.4334 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "333/333 [==============================] - 281s 843ms/step - loss: 0.3215 - val_loss: 0.3584 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "333/333 [==============================] - 283s 849ms/step - loss: 0.2453 - val_loss: 0.3086 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "333/333 [==============================] - 310s 931ms/step - loss: 0.1781 - val_loss: 0.2640 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "333/333 [==============================] - 305s 917ms/step - loss: 0.1292 - val_loss: 0.2334 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "333/333 [==============================] - 306s 918ms/step - loss: 0.0945 - val_loss: 0.2088 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "333/333 [==============================] - 304s 912ms/step - loss: 0.0709 - val_loss: 0.1916 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "333/333 [==============================] - 295s 885ms/step - loss: 0.0576 - val_loss: 0.1827 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "333/333 [==============================] - 304s 912ms/step - loss: 0.0447 - val_loss: 0.1749 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "333/333 [==============================] - 300s 902ms/step - loss: 0.0365 - val_loss: 0.1697 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "333/333 [==============================] - 298s 894ms/step - loss: 0.0306 - val_loss: 0.1688 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "333/333 [==============================] - 299s 898ms/step - loss: 0.0267 - val_loss: 0.1657 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "333/333 [==============================] - 309s 927ms/step - loss: 0.0236 - val_loss: 0.1639 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "333/333 [==============================] - 294s 884ms/step - loss: 0.0232 - val_loss: 0.1624 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "333/333 [==============================] - 311s 935ms/step - loss: 0.0191 - val_loss: 0.1627 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "333/333 [==============================] - 279s 837ms/step - loss: 0.0173 - val_loss: 0.1641 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "333/333 [==============================] - ETA: 0s - loss: 0.0152\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0009800000465475024.\n",
      "333/333 [==============================] - 279s 836ms/step - loss: 0.0152 - val_loss: 0.1630 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "333/333 [==============================] - 279s 837ms/step - loss: 0.0136 - val_loss: 0.1634 - lr: 9.8000e-04\n",
      "Epoch 22/100\n",
      "333/333 [==============================] - 278s 836ms/step - loss: 0.0123 - val_loss: 0.1633 - lr: 9.8000e-04\n",
      "Wall time: 1h 53min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history=model.fit([source_seq_input_train, target_seq_input_train],target_seq_ouput_train, epochs=100,batch_size=16,validation_split = 0.1,callbacks=[reduce_lr,early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "enhanced-protein",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_sentence):\n",
    "\n",
    "  input_sequence=tokenizer_source.texts_to_sequences([input_sentence])\n",
    "  inputs=pad_sequences(input_sequence, maxlen=max_len, padding='post')\n",
    "  inputs=tf.convert_to_tensor(inputs)\n",
    "  result=''\n",
    "  units=128\n",
    "  hidden=[tf.zeros((1,units))]\n",
    "  encoder_output,hidden_state,cell_state=model.encoder(inputs)\n",
    "  dec_hidden=hidden_state\n",
    "  dec_input=tf.expand_dims([tokenizer_target.word_index['<start>']],0)\n",
    "  for t in range(40):\n",
    "      predictions,dec_hidden,cell_state,attention_weights,context_vector=model.decoder.onestepdecoder((dec_input,encoder_output,dec_hidden,cell_state))\n",
    "\n",
    "      predicted_id=tf.argmax(predictions[0]).numpy()\n",
    "      result+=tokenizer_target.index_word[predicted_id]+' '\n",
    "      if tokenizer_target.word_index['<end>']==predicted_id:\n",
    "          return result\n",
    "      dec_input= tf.expand_dims([predicted_id],0)\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "driven-guide",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Toysarus? The place that sell balloon? Merely no point. Queensway? Ar you at Penisula?\n",
      "Prediction: The committees promoting in the mambo stuff is cheap Torquay is at 2 <end> \n",
      "Actual: Toysarus? The place that sell balloon? Merely no point. Queensway? Ar you at Penisula?<end>\n",
      "****************************************************************************************************\n",
      "Input: 1: 15pm. Reached about.\n",
      "Prediction: The place balloons But <end> \n",
      "Actual: 1: 15pm. Reached about.<end>\n",
      "****************************************************************************************************\n",
      "Input: Really only today? Topshop and miss self ridgeline likewise got store wide deduction. ..\n",
      "Prediction: Yes Both of ' s later later later later later later <end> \n",
      "Actual: Really only today? Topshop and miss self ridgeline likewise got store wide deduction. ..<end>\n",
      "****************************************************************************************************\n",
      "Input: Do you want to come to my? But I got to after school, while only. ' not fall at this time, ' m enough.\n",
      "Prediction: How do you ' t buy sandals How ' t buy sandals How ' t buy sandals How ' m buy sandals How ' <end> \n",
      "Actual: Do you want to come to my? But I got to after school, while only. ' not fall at this time, ' m enough.<end>\n",
      "****************************************************************************************************\n",
      "Input: We are outside.\n",
      "Prediction: Where are you <end> \n",
      "Actual: We are outside.<end>\n",
      "****************************************************************************************************\n",
      "Input: You got add me or not? Ane can insure you.\n",
      "Prediction: You have have you have to the wrong you been <end> \n",
      "Actual: You got add me or not? Ane can insure you.<end>\n",
      "****************************************************************************************************\n",
      "Input: But I ' t look fringe! . well, ' s cut. So you cut, we and dye our hair together? Good luck your papers!\n",
      "Prediction: But Tuesday I ' s normal your timetable Anyway ' s normal will the same Audrey them with with with our and <end> \n",
      "Actual: But I ' t look fringe! . well, ' s cut. So you cut, we and dye our hair together? Good luck your papers!<end>\n",
      "****************************************************************************************************\n",
      "Input: Buckeye state no, that means you aren ' t hail for statistic? Then you can ' t help pine tree state print, because I need it right after that. Okay, Entirely the best for you test! Don ' t worry.\n",
      "Prediction: Grounded but I ' m buy sandals How ' t buy always send the wrong then ' s normal normal normal normal normal normal normal ' t bother tester And how ' s normal will to out about ' s \n",
      "Actual: Buckeye state no, that means you aren ' t hail for statistic? Then you can ' t help pine tree state print, because I need it right after that. Okay, Entirely the best for you test! Don ' t worry.<end>\n",
      "****************************************************************************************************\n",
      "Input: Don act stupid !\n",
      "Prediction: wasn ' s <end> \n",
      "Actual: Don't act stupid!<end>\n",
      "****************************************************************************************************\n",
      "Input: What are you eating?\n",
      "Prediction: are are you you <end> \n",
      "Actual: What are you eating?<end>\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,10):\n",
    "    input_sentence=test[\"corupted_text\"].iloc[i]\n",
    "    print('Input:',input_sentence)\n",
    "    print('Prediction:',predict(input_sentence))\n",
    "    print('Actual:',test[\"normal_text_output\"].iloc[i])\n",
    "    print('*'*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
